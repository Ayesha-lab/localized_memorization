{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (28, 28, 1)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "# y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
    "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_defaults = [tf.constant([], dtype=tf.float32)] * 10\n",
    "\n",
    "y_train = tf.data.experimental.CsvDataset(\n",
    "    filenames=[\"noisy_y_train.csv\"], record_defaults=record_defaults\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the CsvDataset to a list of EagerTensors\n",
    "y_train = [tf.convert_to_tensor(value) for value in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(name=\"SGD\")\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32,\n",
    "            (5, 5),\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            input_shape=input_shape,\n",
    "            name=\"conv1_1\",\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            32, (5, 5), padding=\"same\", activation=\"relu\", name=\"conv1_2\"\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            64, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv2_1\"\n",
    "        ),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            64, (3, 3), padding=\"same\", activation=\"relu\", name=\"conv2_2\"\n",
    "        ),\n",
    "        tf.keras.layers.MaxPool2D(strides=(2, 2)),\n",
    "        tf.keras.layers.Dropout(0.25),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", name=\"FC_1\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", name=\"FC_2\"),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"FC_softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(name=\"SGD\"),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gradients from the checkpoint file\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "checkpoint.restore(\"epoch_layer_gradients.ckpt-1\").expect_partial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices with noisy data\n",
    "indices = (np.loadtxt(\"noisy_indices.csv\", delimiter=\",\")).astype(int)\n",
    "indices = indices.tolist()\n",
    "\n",
    "# convert eagertensor to numpy\n",
    "y_train_numpy = [value.numpy() for value in y_train]\n",
    "\n",
    "# create a sample of 1000 values using the 6k noisy examples\n",
    "noisy_values_y = [y_train_numpy[i] for i in indices]\n",
    "noisy_sample_y = noisy_values_y[:6000:6]\n",
    "\n",
    "noisy_values_x = [x_train[i] for i in indices]\n",
    "noisy_sample_x = noisy_values_x[:6000:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clean indices\n",
    "total_indices = list(range(60000))\n",
    "clean_indices = [value for value in total_indices if value not in indices]\n",
    "\n",
    "# create a sample of 1000 values using the remaining 54k clean examples\n",
    "clean_values_y = [y_train_numpy[i] for i in clean_indices]\n",
    "clean_sample_y = clean_values_y[:54000:54]\n",
    "\n",
    "clean_values_x = [x_train[i] for i in clean_indices]\n",
    "clean_sample_x = clean_values_x[:54000:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(noisy_values_x[0].shape)\n",
    "noisy_sample = zip(noisy_sample_x, noisy_sample_y)\n",
    "clean_sample = zip(clean_sample_x, clean_sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_xTensors = tf.convert_to_tensor(noisy_sample_x)\n",
    "clean_xTensors = tf.convert_to_tensor(clean_sample_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change clean to noisy if needed\n",
    "original_pred = model(clean_xTensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_noisy_pred(sample):\n",
    "    outputshape = (1000, 10, 1)\n",
    "    output = tf.zeros(outputshape, dtype=tf.float32)\n",
    "    # add noise to input a make a prediction\n",
    "    for i in range(0, 5):\n",
    "        noise = np.random.normal(size=tf.shape(sample))\n",
    "        noisy_images = sample + noise\n",
    "\n",
    "        # pass to output for averaging\n",
    "        output = output + model(noisy_images)\n",
    "\n",
    "    return output / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_noisy_output = avg_noisy_pred(clean_xTensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import CategoricalAccuracy\n",
    "\n",
    "num_epochs = 1\n",
    "accuracy_metric = CategoricalAccuracy()\n",
    "\n",
    "epoch_gradients = []\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "    for batch_start in range(0, len(x_train), batch_size):\n",
    "        # Extract a batch of data\n",
    "        # X_batch = x_train[batch_start:batch_start + batch_size]\n",
    "        # y_batch = y_train[batch_start:batch_start + batch_size]\n",
    "\n",
    "        # Forward pass and compute gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            # predictions = model(X_batch)\n",
    "\n",
    "            predictions = avg_noisy_output\n",
    "\n",
    "            loss = cce(y_batch, predictions)\n",
    "            loss_value = tf.reduce_mean(loss)\n",
    "\n",
    "        # Backward pass and update weights\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "        # optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        # Get accuracy of the epoch\n",
    "        accuracy_metric.update_state(y_batch, predictions)\n",
    "\n",
    "    accuracy = accuracy_metric.result()\n",
    "    accuracy_metric.reset_states()  # reset for next epoch\n",
    "\n",
    "    epoch_gradients.append(gradients)\n",
    "\n",
    "    print(f\"  Loss: {loss_value.numpy()}, Acc: {accuracy.numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
